{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "eShgn9S30Xid",
        "outputId": "8c9ddd27-1a99-4033-8581-091a9548dc1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.10.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.0.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "<ipython-input-4-7c3e5862a308>:50: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use(\"seaborn-whitegrid\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install emoji\n",
        "!pip install langdetect\n",
        "!pip install contractions\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "# Text cleaning\n",
        "import re\n",
        "import string\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from langdetect import detect, LangDetectException\n",
        "import contractions\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Transformers library for BERT\n",
        "import transformers\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import time\n",
        "\n",
        "# Set seed for reproducibility\n",
        "import random\n",
        "seed_value = 2042\n",
        "random.seed(seed_value)\n",
        "np.random.seed(seed_value)\n",
        "torch.manual_seed(seed_value)\n",
        "torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.despine()\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "plt.rc(\"figure\", autolayout=True)\n",
        "plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n",
        "\n",
        "# Define stop words for text cleaning\n",
        "stop_words = set(stopwords.words('english'))\n",
        "batch_size=32\n",
        "# Initialize lemmatizer for text cleaning\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "profanity_txt=open('/content/drive/MyDrive/FYP/profanityList.txt','r')\n",
        "profanity_txt=profanity_txt.read().strip()\n",
        "profanity_list=profanity_txt.split('\\n')\n",
        "\n",
        "def highlight(text):\n",
        "    highlighted_text = text\n",
        "\n",
        "    for profane_word in profanity_list:\n",
        "        pattern = r'\\b{}\\b'.format(re.escape(profane_word))\n",
        "        highlighted_text = re.sub(pattern, '\\033[91m{}\\033[0m'.format(profane_word), highlighted_text, flags=re.IGNORECASE)\n",
        "\n",
        "    return highlighted_text\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "def bert_tokenizer(data):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]` special tokens\n",
        "            max_length=128,             # Choose max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class Bert_Classifier(nn.Module):\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        super(Bert_Classifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of the classifier, and number of labels\n",
        "        n_input = 768\n",
        "        n_hidden = 50\n",
        "        n_output = 6\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Instantiate the classifier (a fully connected layer followed by a ReLU activation and another fully connected layer)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(n_input, n_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_hidden, n_output)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model weights if freeze_bert is True (useful for feature extraction without fine-tuning)\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Feed input data (input_ids and attention_mask) to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the `[CLS]` token from the BERT output (useful for classification tasks)\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed the extracted hidden state to the classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Instantiate an instance of your model\n",
        "model = Bert_Classifier(freeze_bert=False)\n",
        "\n",
        "model_path = '/content/drive/MyDrive/FYP/bertmodel.pth'\n",
        "\n",
        "# Load the model on CPU\n",
        "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu'))['model_state_dict'])\n",
        "\n",
        "class_id_to_label={0:'age',1:'ethnicity',2:'gender',3:'not_cyberbullying',4:'other_cyberbullying',5:'religion'}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"smiteis on X: @XraigFree @hurrica45391237 @InvalidDimensi1 An example: everyone in school growing up during that period had gay, rape, murder, STD, etc jokes in their back pocket because that’s what got laughs. It wasn’t cool then and it isn’t cool now, but that’s how things were. / X (twitter.com)\"\n",
        "test_inputs, test_masks = bert_tokenizer([sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34OjA83Z1y4G",
        "outputId": "c2ef4b84-a576-4b49-a7d6-46e24641e586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentenceCheck = highlight(sentence)\n",
        "\n",
        "model.eval()\n",
        "logits = model(test_inputs, test_masks)\n",
        "\n",
        "probs = nn.functional.softmax(logits, dim=-1)\n",
        "predicted_class = torch.argmax(probs, dim=-1)\n",
        "\n",
        "predicted_label = class_id_to_label[predicted_class.item()]\n",
        "print(sentenceCheck)\n",
        "print()\n",
        "print(\"Predicted class:\", predicted_label)\n",
        "\n",
        "probabilitys=probs.tolist()[0]\n",
        "for idx,prob in enumerate(probabilitys):\n",
        "  print(class_id_to_label[idx],\" : \",prob*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwNb_74c15Gh",
        "outputId": "8c51d3e2-ca93-4852-b724-40dd1d8bd3e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smiteis on X: @XraigFree @hurrica45391237 @InvalidDimensi1 An example: everyone in school growing up during that period had gay, \u001b[91mrape\u001b[0m, murder, STD, etc jokes in their back pocket because that’s what got laughs. It wasn’t cool then and it isn’t cool now, but that’s how things were. / X (twitter.com)\n",
            "\n",
            "Predicted class: gender\n",
            "age  :  13.765475153923035\n",
            "ethnicity  :  1.4450113289058208\n",
            "gender  :  71.86416387557983\n",
            "not_cyberbullying  :  10.500755906105042\n",
            "other_cyberbullying  :  1.9156375899910927\n",
            "religion  :  0.5089579150080681\n"
          ]
        }
      ]
    }
  ]
}